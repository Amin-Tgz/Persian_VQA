{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "parsi_vqa_1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PgOjabLf0Epy",
        "m4OteN2u22vj"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maryamhashemi/Persian_VQA/blob/master/parsi_vqa_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBWMpHGoxrnz",
        "colab_type": "text"
      },
      "source": [
        "### Import Prerequesties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK-PVH7YP_L4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install arabic_reshaper \n",
        "!pip install python-bidi\n",
        "!pip install deepdish\n",
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade tables\n",
        "!pip install hazm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6bRyFIkFhGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os,h5py\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import deepdish as dd\n",
        "from PIL import Image\n",
        "import arabic_reshaper\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from bidi.algorithm import get_display\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding, Multiply, Input\n",
        "\n",
        "import random as python_random\n",
        "import tensorflow as tf\n",
        "import math\n",
        "from tensorflow.keras.callbacks import EarlyStopping \n",
        "import tensorflow as tf\n",
        "import hazm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOGILD3IxLAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f73bd1de-f144-4d53-8123-143cbfa4c190"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-5xC8j0y1xk",
        "colab_type": "text"
      },
      "source": [
        "### Set Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVebfTOqy64y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DROPOUT_RATE = 0.5\n",
        "EMBEDDING_DIM = 300\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 256\n",
        "SEQ_LENGTH = 26 #100\n",
        "VOCAB_SIZE = 1000\n",
        "OOV_TOK = \"<OOV>\"\n",
        "\n",
        "BASE_PATH = '/content/drive/My Drive/Persian_VQA/dataset/'\n",
        "QUESTION_TRAIN_PATH =   os.path.join(BASE_PATH, 'google-train_questions.json')#OpenEnded_mscoco_train2014_questions.json\n",
        "ANNOTATION_TRAIN_PATH = os.path.join(BASE_PATH, 'mscoco_train2014_annotations.json')\n",
        "# IMAGE_TRAIN_PATH = os.path.join(BASE_PATH, 'train_images_1000')\n",
        "IMAGE_TRAIN_PATH = os.path.join('/content/', 'train')\n",
        "\n",
        "QUESTION_VAL_PATH =   os.path.join(BASE_PATH, 'google-val_questions.json')#'OpenEnded_mscoco_val2014_questions.json'\n",
        "ANNOTATION_VAL_PATH = os.path.join(BASE_PATH, 'mscoco_val2014_annotations.json')\n",
        "# IMAGE_VAL_PATH = os.path.join(BASE_PATH, 'val_images_500')\n",
        "IMAGE_VAL_PATH = os.path.join('/content/', 'val')\n",
        "\n",
        "QUESTION_TEST_PATH =   os.path.join(BASE_PATH, '...')\n",
        "ANNOTATION_TEST_PATH = os.path.join(BASE_PATH, '...')\n",
        "# IMAGE_TEST_PATH = os.path.join(BASE_PATH, 'val_images_500')\n",
        "IMAGE_TEST_PATH = os.path.join('/content/', 'test')\n",
        "\n",
        "NUM_OF_CLASSES=1000\n",
        "NUM_OF_MOST_COMMON_ANSWERS = 999#999\n",
        "BASE_PATH_parssoft = '/content/drive/My Drive/parssoftco_PVQA/'\n",
        "BASE_PATH_parssoft2 = '/content/drive/My Drive/parssoftco_PVQA2/'\n",
        "BASE_PATH_parssoft3 = '/content/drive/My Drive/parssoftco_PVQA3/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJg7-dXsa8i_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "from zipfile import ZipFile\n",
        "# #If the downloaded file is a zip file than you can use below function to unzip it.\n",
        "def unzip(dir,where):\n",
        "    with ZipFile(dir) as zipf:\n",
        "        zipf.extractall(where)\n",
        "    print(\"File Unzipped!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqyLy7lGxky7",
        "colab_type": "text"
      },
      "source": [
        "### Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFFjq4jWHgKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def most_common_answer_from_train(answers,num_of_common_answers):\n",
        "  counts = {}\n",
        "  for ans in answers:\n",
        "      counts[ans] = counts.get(ans,0) + 1\n",
        "  counter = sorted([(count,w) for w,count in counts.items()], reverse=True)\n",
        "  most_common_vocab = []\n",
        "  for i in range(num_of_common_answers):\n",
        "      most_common_vocab.append(counter[i][1])\n",
        "  return most_common_vocab#,counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFw0Yqw8NWJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_train_dataset(qus, ann, answertoindex):\n",
        "  qs = []\n",
        "  # raw_answers = []\n",
        "  answers = []\n",
        "  qs_id = []\n",
        "  im_id = []\n",
        "\n",
        "  # ann2=ann\n",
        "  # for ann in ann['annotations']:\n",
        "  #   raw_answers.append(ann['multiple_choice_answer'])\n",
        "  # most_common_answers = most_common_answer_from_train(raw_answers,NUM_OF_MOST_COMMON_ANSWERS)\n",
        "  # answertoindex = {w:i+1 for i,w in enumerate(most_common_answers)}\n",
        "  # indextoanswer = {i+1:w for i,w in enumerate(most_common_answers)}\n",
        "\n",
        "  filtered_train_question_ids={}\n",
        "  # ann=ann2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  i=0\n",
        "  for ann in ann['annotations']:\n",
        " \n",
        "    if ann['multiple_choice_answer'] in answertoindex.keys():\n",
        "      answers.append(ann['multiple_choice_answer'])\n",
        "      # filtered_train_question_ids.append(ann['question_id'])\n",
        "      filtered_train_question_ids[ann['question_id']]=1\n",
        "      # if(i%1000==0):print(i)\n",
        "      i+=1\n",
        "\n",
        "  for q in qus['questions']:\n",
        "    if q['question_id'] in filtered_train_question_ids.keys():\n",
        "      qs.append(q['question'])\n",
        "      qs_id.append(q['question_id'])\n",
        "      im_id.append(q['image_id'])\n",
        "\n",
        "\n",
        "  return qs, answers, qs_id, im_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJBqQMXBwtuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(qus, ann):\n",
        "  qs = []\n",
        "  raw_answers = []\n",
        "  answers = []\n",
        "  qs_id = []\n",
        "  im_id = []\n",
        " \n",
        "\n",
        "  for q in qus['questions']:\n",
        "    qs.append(q['question'])\n",
        "    qs_id.append(q['question_id'])\n",
        "    im_id.append(q['image_id'])\n",
        "\n",
        "  for ann in ann['annotations']:\n",
        "    answers.append(ann['multiple_choice_answer'])\n",
        "\n",
        "\n",
        "  return qs, answers, qs_id, im_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTqIiXJnK6Uv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_train_dataset():\n",
        "  qs = json.load( open(QUESTION_TRAIN_PATH))\n",
        "  ann = json.load( open(ANNOTATION_TRAIN_PATH))\n",
        "\n",
        "  original_ann = ann\n",
        "  raw_answers = []\n",
        "  for ann in ann['annotations']:\n",
        "    raw_answers.append(ann['multiple_choice_answer'])\n",
        "  most_common_answers = most_common_answer_from_train(raw_answers,NUM_OF_MOST_COMMON_ANSWERS)#NUM_OF_MOST_COMMON_ANSWERS\n",
        "  answertoindex = {w:i for i,w in enumerate(most_common_answers)}#i+1\n",
        "  indextoanswer = {i:w for i,w in enumerate(most_common_answers)}#i+1\n",
        "\n",
        "  qs, answers, qs_id, im_id=create_train_dataset(qs, original_ann, answertoindex)\n",
        "\n",
        "  return qs, answers, qs_id, im_id,answertoindex,indextoanswer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jenrLGl03rdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_val_dataset():\n",
        "  qs = json.load( open(QUESTION_VAL_PATH))\n",
        "  ann = json.load( open(ANNOTATION_VAL_PATH))\n",
        "  \n",
        "  return create_dataset(qs, ann)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ6eQ8HOPI1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_test_dataset():\n",
        "  qs = json.load( open(QUESTION_TEST_PATH))\n",
        "  # ann = json.load( open(ANNOTATION_TRAIN_PATH))\n",
        "  \n",
        "  return create_dataset(ims, qs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2zFZ8s2Wp2p",
        "colab_type": "text"
      },
      "source": [
        "#### Visualize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV5MQ5WE_E0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_visualqa(qs, answer, image):\n",
        "  im = Image.open(image)  \n",
        "  plt.figure()\n",
        "  plt.imshow(im)\n",
        "  title = arabic_reshaper.reshape(qs + \"\\n\" + answer)\n",
        "  title = get_display(title) \n",
        "  plt.title(title)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "def visualize_train(num):\n",
        "  show_visualqa(train_qs[num], train_answers[num], ims.get(train_image_ids[num]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aZM3F6EH3PE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "d6241cb1-7ac2-4120-9501-8afd0e9fccdb"
      },
      "source": [
        "train_qs, train_answers, train_q_ids, train_image_ids, answertoindex, indextoanswer = get_train_dataset()\n",
        "val_qs, val_answers, val_q_ids, val_image_ids = get_val_dataset()\n",
        "# test_qs, test_answers, test_q_ids, test_image_ids = get_test_dataset()\n",
        "\n",
        "print('train : ')\n",
        "print(len(train_qs))\n",
        "print(len(train_answers))\n",
        "print(len(train_q_ids))\n",
        "print(len(train_image_ids))\n",
        "\n",
        "print('val : ')\n",
        "print(len(val_qs))\n",
        "print(len(val_answers))\n",
        "print(len(val_q_ids))\n",
        "print(len(val_image_ids))\n",
        "\n",
        "# print(len(test_qs))\n",
        "# print(len(test_answers))\n",
        "# print(len(test_q_ids))\n",
        "# print(len(test_image_ids))\n",
        "\n",
        "#train : 215359   val: 121512\n",
        "#train : 215375   val: 121512 ours"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train : \n",
            "215359\n",
            "215359\n",
            "215359\n",
            "215359\n",
            "val : \n",
            "121512\n",
            "121512\n",
            "121512\n",
            "121512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T80BONcg1fTp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6d617b7-2544-4945-b103-7da6e0a0c85d"
      },
      "source": [
        "train_q_ids[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4870251"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "von-rAbS1jz0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1155bf15-1131-4d04-c5cb-f4b4857a8556"
      },
      "source": [
        "train_image_ids[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "487025"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UQ_b7BJeio9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(BASE_PATH_parssoft+'different_qidsto1000.npy',list(set(train_q_ids) - set(train_q_ids_old)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlzDNNBr7RIh",
        "colab_type": "text"
      },
      "source": [
        "#### Prepare questions to feed into network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWQ4jKhkzimO",
        "colab_type": "text"
      },
      "source": [
        "##### Downloads and unzips"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z_h_urH1Y31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
        "!wget https://github.com/HaniehP/PersianNER/blob/master/glove300d.txt.zip #persian glove"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASeCtAsBFP3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unzip(BASE_PATH_parssoft3+'glove300d.txt.zip','/content/')#'/content/drive/My Drive/parssoftco_PVQA/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3dTQ7MhRyeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip /content/glove.6B.zip\n",
        "# unzip('/content/glove.6B.zip','/content/')#'/content/drive/My Drive/parssoftco_PVQA/'\n",
        "unzip(BASE_PATH_parssoft3+'glove300d.txt.zip','/content/')#'/content/drive/My Drive/parssoftco_PVQA/'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGzP7nNujxhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlopen\n",
        "import gzip\n",
        "\n",
        "vocab_and_vectors = {}\n",
        "file = open('glove300d.txt')#BASE_PATH_parssoft+'glove.6B.300d.txt'\n",
        "for line in (file):\n",
        "    value = line.split(' ')\n",
        "    word = value[0]\n",
        "    coef = np.array(value[1:],dtype = 'float32')\n",
        "    vocab_and_vectors[word] = coef\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO6vIV6FW_kL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "09bd29e5-9327-44ee-d554-2e08a54fc66f"
      },
      "source": [
        "\n",
        "\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-08 22:28:00--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1258183862 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.fa.300.vec.gz’\n",
            "\n",
            "cc.fa.300.vec.gz    100%[===================>]   1.17G  11.4MB/s    in 1m 47s  \n",
            "\n",
            "2020-08-08 22:29:48 (11.2 MB/s) - ‘cc.fa.300.vec.gz’ saved [1258183862/1258183862]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCA_NXVdXHXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gunzip /content/cc.fa.300.vec.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vfRTWXRIanN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fd252778-0f53-4774-cc04-0bc4c794ebd2"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-05 19:38:37--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.en.300.vec.gz’\n",
            "\n",
            "cc.en.300.vec.gz    100%[===================>]   1.23G  9.86MB/s    in 2m 4s   \n",
            "\n",
            "2020-08-05 19:40:43 (10.2 MB/s) - ‘cc.en.300.vec.gz’ saved [1325960915/1325960915]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ew-TPnOIhPC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1b8339e-5e72-4022-84c2-1c478b3cd12e"
      },
      "source": [
        "!gunzip /content/cc.en.300.vec.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gzip: /content/cc.en.300.vec.gz: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64Mk45bIKCod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/cc.fa.300.vec /content/drive/My\\ Drive/parssoftco_PVQA3/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTeDwyR9Lc26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = open('cc.fa.300.vec', encoding=\"utf8\")#BASE_PATH_parssoft3+'cc.en.300.vec'\n",
        "vocab_and_vectors = {}\n",
        "# put words as dict indexes and vectors as words values\n",
        "for line in file:\n",
        "  values = line.split()\n",
        "  word = values [0]#.decode('utf-8')\n",
        "  vector = np.asarray(values[1:], dtype='float32')\n",
        "  vocab_and_vectors[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve6QIMugr6_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlopen\n",
        "import gzip\n",
        "\n",
        "# get the vectors\n",
        "file = gzip.open(urlopen('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz'))\n",
        "# https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
        "# https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "# Now let’s prepare this file for vector extraction.\n",
        "\n",
        "vocab_and_vectors = {}\n",
        "# put words as dict indexes and vectors as words values\n",
        "for line in file:\n",
        "  values = line.split()\n",
        "  word = values[0].decode('utf-8')\n",
        "  vector = np.asarray(values[1:], dtype='float32')\n",
        "  vocab_and_vectors[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHHxWq68GMvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dd.io.save(BASE_PATH_parssoft +'fasttext-en-300.h5', vocab_and_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW7vaKrFB7EK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_and_vectors=dd.io.load(BASE_PATH_parssoft +'fasttext-en-300.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwsFvm_QqqVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#persian fasttext\n",
        "from urllib.request import urlopen\n",
        "import gzip\n",
        "\n",
        "# get the vectors\n",
        "file_fa = gzip.open(urlopen('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz'))\n",
        "# https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
        "# Now let’s prepare this file for vector extraction.\n",
        "\n",
        "\n",
        "\n",
        "# import codecs\n",
        "# f = codecs.open('unicode.rst', encoding='utf-8')\n",
        "# for line in f:\n",
        "#     print (repr(line))\n",
        "\n",
        "vocab_and_vectors_fa = {}\n",
        "# put words as dict indexes and vectors as words values\n",
        "for line in file_fa:\n",
        "  values = line.split()\n",
        "  # print(values)\n",
        "  word = values [0].decode('utf-8')\n",
        "  vector = np.asarray(values[1:], dtype='float32')\n",
        "  vocab_and_vectors_fa[word] = vector\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05HFXksgz1AO",
        "colab_type": "text"
      },
      "source": [
        "##### making texts and embedding ready "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8teUMJy7aWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "normalizer = hazm.Normalizer()\n",
        "# num_words = VOCAB_SIZE,\n",
        "# train_qs = [item.replace('؟',' ؟') for item in train_qs]\n",
        "# val_qs = [item.replace('؟',' ؟') for item in val_qs]\n",
        "\n",
        "train_qs = [normalizer.normalize(item) for item in train_qs]\n",
        "val_qs = [normalizer.normalize(item) for item in val_qs]\n",
        "\n",
        "\n",
        "# !\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n\n",
        "tokenizer = Tokenizer(oov_token=OOV_TOK)#filters=\"-.\\\"',:? !\\$#@~()*&\\^%;\\[\\]/\\\\\\+<>\\n=\"\n",
        "tokenizer.fit_on_texts(train_qs)\n",
        "\n",
        "train_X_seqs = tokenizer.texts_to_sequences(train_qs)\n",
        "val_X_seqs = tokenizer.texts_to_sequences(val_qs)\n",
        "# test_X_seqs = tokenizer.texts_to_sequences(test_qs)\n",
        "\n",
        "train_X_seqs = pad_sequences(train_X_seqs, maxlen=SEQ_LENGTH, padding='pre')#post\n",
        "val_X_seqs = pad_sequences(val_X_seqs, maxlen=SEQ_LENGTH, padding='pre')#post\n",
        "# test_X_seqs = pad_sequences(test_X_seqs, maxlen=SEQ_LENGTH, padding='post')\n",
        "\n",
        "train_X_seqs = np.array(train_X_seqs)\n",
        "val_X_seqs = np.array(val_X_seqs)\n",
        "# test_X_seqs = np.array(test_X_seqs)\n",
        "\n",
        "\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPNPGlbBk2-V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d9a974a5-b0b3-4313-ec6a-6b4827e52f43"
      },
      "source": [
        "len(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13191"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-qsiRUy0Sts",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c2fe3f67-fb33-4a35-d606-1d324a1d341b"
      },
      "source": [
        "val_qs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'میز ساخته شده از چیست؟'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq7kN4El0B7-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3ccef633-9e7a-4682-82f8-ef96373b3bd6"
      },
      "source": [
        "val_X_seqs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0, 84, 66, 19, 17, 11,  2], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyKKCnmGnGH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_X_seqs = [word_tokenize(i) for i in train_qs]\n",
        "# val_X_seqs = [word_tokenize(i) for i in val_qs]\n",
        "\n",
        "# all_words = []\n",
        "# for sent in train_qs:\n",
        "#     tokenize_word = word_tokenize(sent)\n",
        "#     for word in tokenize_word:\n",
        "#         all_words.append(word)\n",
        "# from keras.preprocessing.text import one_hot\n",
        "# embedded_sentences = [one_hot(sent, 13447) for sent in train_qs]\n",
        "# print(embedded_sentences[0] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-3CwheBNTV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def right_align(seq,lengths):\n",
        "    v = np.zeros(np.shape(seq))\n",
        "    N = np.shape(seq)[1]\n",
        "    for i in range(np.shape(seq)[0]):\n",
        "        v[i][N-lengths[i]:N]=seq[i][0:lengths[i]]\n",
        "    return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERvAF5fhXyCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ques_data = h5py.File(BASE_PATH_parssoft + 'data_prepro.h5')\n",
        "ques_answers = np.array(ques_data['answers'])[:]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaSsNE_nYIFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ques_answers.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d5rU2I0YA_L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf26e140-b505-4568-9d9f-16f164a840f1"
      },
      "source": [
        "for i,q in enumerate(ques_train):\n",
        "  print(i)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgOjabLf0Epy",
        "colab_type": "text"
      },
      "source": [
        "##### Making english texts ready for default paper settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvMvhCiWhr3A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8a766ce5-7590-4fb7-b797-2ea2878698ad"
      },
      "source": [
        "qss_train = json.load( open(QUESTION_TRAIN_PATH))\n",
        "qss_val = json.load( open(QUESTION_VAL_PATH))\n",
        "ques_data = h5py.File(BASE_PATH_parssoft + 'data_prepro.h5')\n",
        "qid_to_tokens_train={}\n",
        "\n",
        "ques_train = np.array(ques_data['ques_train'])[:, :]\n",
        "ques_length_train = np.array(ques_data['ques_length_train'])[:]\n",
        "ques_train = right_align(ques_train, ques_length_train)\n",
        "qids_train=ques_data['question_id_train'][:]\n",
        "qid_to_tokens_train={}\n",
        "for i,q in enumerate(ques_train):\n",
        "  qid_to_tokens_train[qids_train[i]]=q\n",
        "\n",
        "ques_val = np.array(ques_data['ques_test'])[:, :]\n",
        "ques_length_val = np.array(ques_data['ques_length_test'])[:]\n",
        "ques_val = right_align(ques_val, ques_length_val)\n",
        "\n",
        "for _ in ques_val:\n",
        "    if 12602 in _:\n",
        "        _[_==12602] = 0\n",
        "\n",
        "qids_val=ques_data['question_id_test'][:]\n",
        "qid_to_tokens_val={}\n",
        "for i,q in enumerate(ques_val):\n",
        "  qid_to_tokens_val[qids_val[i]]=q\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq-v9MMbyNMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X_seqs = np.array([qid_to_tokens_train[id] for id in train_q_ids])\n",
        "val_X_seqs = np.array([qid_to_tokens_val[id] for id in val_q_ids])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b8_Sbk303Dw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_metadata():\n",
        "    meta_data = json.load(open(BASE_PATH_parssoft+'data_prepro.json', 'r'))\n",
        "    meta_data['ix_to_word'] = {str(word):int(i) for i,word in meta_data['ix_to_word'].items()}\n",
        "    return meta_data\n",
        "\n",
        "metadata = get_metadata()\n",
        "num_classes = len(metadata['ix_to_ans'].keys())\n",
        "num_words = len(metadata['ix_to_word'].keys())\n",
        "\n",
        "# anss = len(metadata['ans_to_ix'].keys())\n",
        "\n",
        "# embedding_matrix = np.zeros((num_words, 300))\n",
        "word_index = metadata['ix_to_word']\n",
        "\n",
        "# for word, i in word_index.items():\n",
        "#     embedding_vector = vocab_and_vectors.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "\n",
        "# print(num_classes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoTU8Lnzacnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metadata['ix_to_ans'].keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkjjxx9fbQrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(word_index.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9N5ZPKjyFsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_X_ims = [train_images_dict[id] for id in train_image_ids]\n",
        "# val_X_ims = [val_images_dict[id] for id in val_image_ids]\n",
        "# train_X_ims = np.array(train_X_ims)\n",
        "# val_X_ims = np.array(val_X_ims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLMmlhwwOWzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g=0\n",
        "for word, i in word_index.items():\n",
        "  print(i,word)\n",
        "  if (i==10):break\n",
        "  g+=1\n",
        "print(g)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sy-2u_i0gAh",
        "colab_type": "text"
      },
      "source": [
        "##### making embedings matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo0lkDKNlM6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for bultin keras tokenizer\n",
        "embedding_matrix = np.zeros((len(word_index)+1 , EMBEDDING_DIM))#+ 1\n",
        "for word, i in word_index.items():\n",
        "  # print(i)\n",
        "  # embedding_vector = vocab_and_vectors_fa.get(word)\n",
        "  # embedding_vector = vocab_and_vectors.get(word)\n",
        "  embedding_vector = vocab_and_vectors.get(word) #glove\n",
        "  \n",
        "  # words that cannot be found will be set to 0\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6GecXkfN1QB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_and_vectors=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yxOUW-smw7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = np.zeros((len(word_index) , EMBEDDING_DIM))#+ 1\n",
        "for word, i in word_index.items():\n",
        "  # print(i)\n",
        "  # embedding_vector = vocab_and_vectors_fa.get(word)\n",
        "  # embedding_vector = vocab_and_vectors.get(word)\n",
        "  embedding_vector = vocab_and_vectors.get(word) #glove\n",
        "  \n",
        "  # words that cannot be found will be set to 0\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5ZgDeyyjJ-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix_old=embedding_matrix\n",
        "print(embedding_matrix_old[0]==embedding_matrix[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D9nZ3z704RJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embedding_ft_t_keras_farsi_hazmnorm_d300_l26\n",
        "# embedding_glv_t_keras_farsi_d300_l26\n",
        "# embedding_ft_t_keras_farsi_d300_l26\n",
        "# embedding_matrix_glove_d200_l26_prepad_orig\n",
        "# embedding_matrix_glove_d300_l26\n",
        "# embedding_matrix_glove_d300_l26_prepad_originalpaper\n",
        "# embedding_matrix_glove_d300_l26_prepad_originalpaper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVI1xS3EZJqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(BASE_PATH_parssoft + 'embedding_glove_t_keras_farsi_hazmnorm_d300_l26_pre',embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF2x34lK0Jxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = np.load(BASE_PATH_parssoft + 'embedding_ft_t_keras_farsi_hazmnorm_d300_l26_pre.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrMIOnHy7bdA",
        "colab_type": "text"
      },
      "source": [
        "#### Prepare images to feed into network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iST5QjfB9-iE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#VGG19 tra\n",
        "train_images_dict = dd.io.load('/content/drive/My Drive/parssoftco_PVQA3/train_full_features_VGG19_dict.h5')\n",
        "# val_images_dict = dd.io.load('/content/drive/My Drive/parssoftco_PVQA3/val_full_features_VGG19_dict.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM6YU7FwI2Zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#VGG19 pre\n",
        "train_images_dict = dd.io.load(BASE_PATH_parssoft+'train_imid_to_feats_dict.h5')\n",
        "val_images_dict = dd.io.load(BASE_PATH_parssoft+'val_imid_to_feats_dict.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Etepo_pK0p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "1a52dfe0-d5a8-4ba1-cc2e-bbf0033a279e"
      },
      "source": [
        "# qss_train = json.load( open(QUESTION_TRAIN_PATH))\n",
        "# qss_val = json.load( open(QUESTION_VAL_PATH))\n",
        "# qid_to_imid_train={}\n",
        "# for q in qss_train['questions']:\n",
        "#   qid_to_imid_train[q['question_id']]=q['image_id']\n",
        "# qid_to_imid_val={}\n",
        "# for q in qss_val['questions']:\n",
        "#   qid_to_imid_val[q['question_id']]=q['image_id']\n",
        "\n",
        "##\n",
        "#features of VGG19\n",
        "# img_data = h5py.File('data_img.h5')\n",
        "# ques_data = h5py.File('data_prepro.h5')\n",
        "\n",
        "# img_data_train = np.array(img_data['images_train'])\n",
        "# img_pos_train = ques_data['img_pos_train'][:]\n",
        "# qids_train=ques_data['question_id_train'][:]\n",
        "# train_img_data = np.array([img_data_train[_-1,:] for _ in img_pos_train])\n",
        "# img_features_dict_train={}\n",
        "# for i,q in enumerate(qids_train):\n",
        "#   if not qid_to_imid_train[q] in img_features_dict_train:\n",
        "#     img_features_dict_train[qid_to_imid_train[q]]=train_img_data[i-1]\n",
        "\n",
        "# img_data_val = np.array(img_data['images_test'])\n",
        "# img_pos_val = ques_data['img_pos_test'][:]\n",
        "# qids_val=ques_data['question_id_test'][:]\n",
        "# val_img_data = np.array([img_data_val[_-1,:] for _ in img_pos_val])\n",
        "# img_features_dict_val={}\n",
        "# for i,q in enumerate(qids_val):\n",
        "#   if not qid_to_imid_val[q] in img_features_dict_val:\n",
        "#     img_features_dict_val[qid_to_imid_val[q]]=val_img_data[i-1]\n",
        "\n",
        "# # dd.io.save(BASE_PATH_parssoft+'train_imid_to_feats_dict.h5',img_features_dict_train)\n",
        "# # dd.io.save(BASE_PATH_parssoft+'val_imid_to_feats_dict.h5',img_features_dict_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RELjtuYPpPvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tem = np.sqrt(np.sum(np.multiply(val_X_ims, val_X_ims), axis=1))\n",
        "t=np.tile(tem,(4096,1))\n",
        "val_X_ims = np.divide(val_X_ims, np.transpose(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRoYCSV2tZNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tem = np.sqrt(np.sum(np.multiply(train_X_ims, train_X_ims), axis=1))\n",
        "t=np.tile(tem,(4096,1))\n",
        "train_X_ims = np.divide(train_X_ims, np.transpose(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsOI-ueCs-yY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X_ims=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO9LTTruMo9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X_ims = [train_images_dict[id] for id in train_image_ids]\n",
        "train_X_ims = np.array(train_X_ims)\n",
        "\n",
        "# val_X_ims = [val_images_dict[id] for id in val_image_ids]\n",
        "# val_X_ims = np.array(val_X_ims)\n",
        "\n",
        "\n",
        "train_images_dict =0\n",
        "val_images_dict =0\n",
        "train_ims =None\n",
        "val_ims =None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm4VToqlAYMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images_dict=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axsQ85wULHCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(BASE_PATH_parssoft+ 'train_X_ims_us_l2_359.npy',train_X_ims)\n",
        "# np.save('val_X_ims_us_l2_359.npy',val_X_ims)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1BXnpvyoKTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/val_X_ims_us_l2_359.npy  /content/drive/My\\ Drive/parssoftco_PVQA/val_X_ims_us_l2_359.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXATIPi2Ajto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_X_ims=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BIbe04rytTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/train_X_ims_us_l2_359.npy  /content/drive/My\\ Drive/parssoftco_PVQA/train_X_ims_us_l2_359.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55FZjAKdCgsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X_ims = np.load(BASE_PATH_parssoft + 'train_X_ims_us_l2_359.npy')#, mmap_mode='r'  train_X_ims_pre_l2\n",
        "val_X_ims = np.load(BASE_PATH_parssoft + 'val_X_ims_us_l2_359.npy', mmap_mode='r')#, mmap_mode='r' val_X_ims_pre_l2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzbbhC5n8CYo",
        "colab_type": "text"
      },
      "source": [
        "#### Prepare labels of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYl1K60q8J87",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2c79b05c-1c83-43d2-a424-2fcafef5ac32"
      },
      "source": [
        "# all_answers = get_answers(train_answers)\n",
        "# num_answers = len(all_answers)\n",
        "# num_classes = num_answers +1 \n",
        "\n",
        "train_answer_indices = []\n",
        "val_answer_indices = []\n",
        "test_answer_indices = []\n",
        "\n",
        "for ans in train_answers:\n",
        "    train_answer_indices.append(answertoindex[ans])\n",
        " \n",
        "\n",
        "for ans in val_answers:\n",
        "  if ans in answertoindex.keys():\n",
        "    val_answer_indices.append(answertoindex[ans])\n",
        "  else: \n",
        "    val_answer_indices.append('999')\n",
        "\n",
        "\n",
        "\n",
        "# for a in test_answers:\n",
        "#   if a in all_answers:\n",
        "#     test_answer_indices.append(all_answers.index(a))\n",
        "#   else:\n",
        "#     test_answer_indices.append(num_answers)\n",
        "\n",
        "train_Y = to_categorical(np.array(train_answer_indices),num_classes=NUM_OF_CLASSES)#NUM_OF_CLASSES-1\n",
        "val_Y = to_categorical(np.array(val_answer_indices))\n",
        "# test_Y = to_categorical(test_answer_indices)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# train_Y = to_categorical(np.array(train_answers).factorize()[0])\n",
        "# val_Y = to_categorical(np.array(val_answers).factorize()[0])\n",
        "# test_Y = to_categorical(test_answer_indices)\n",
        "\n",
        "\n",
        "print(train_Y.shape)\n",
        "print(val_Y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(215359, 1000)\n",
            "(121512, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5OZxbkk9jKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "16ec744e-c5a6-49a7-d6d4-a1173f6d92ea"
      },
      "source": [
        "answertoindex.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['yes', 'no', '2', '1', 'white', '3', 'red', 'blue', '4', 'green', 'black', 'yellow', 'brown', '5', 'tennis', 'baseball', 'right', 'orange', 'bathroom', 'left', '6', 'wood', 'frisbee', '0', 'pink', 'pizza', 'gray', 'kitchen', 'skateboarding', 'cat', '7', 'black and white', 'skiing', '8', 'dog', 'surfing', 'water', 'snow', 'skateboard', '10', 'wii', 'surfboard', 'man', 'kite', 'grass', 'purple', 'giraffe', 'eating', 'broccoli', 'stop', 'elephant', 'phone', 'apple', 'train', 'winter', 'umbrella', 'sheep', 'silver', 'horse', 'banana', 'motorcycle', '9', 'laptop', 'beach', 'sunny', 'cake', 'brick', 'wine', 'woman', 'hat', '12', 'bear', 'flowers', 'food', 'bananas', 'table', 'soccer', 'living room', 'female', 'bench', 'cow', 'bus', 'zebra', 'snowboarding', 'male', 'kites', '11', 'hot dog', 'tennis racket', 'trees', 'helmet', 'night', 'fence', 'teddy bear', 'down', 'tile', 'tan', 'standing', 'camera', 'airport', 'outside', 'bat', 'bird', 'donut', 'cloudy', 'bed', 'christmas', 'zoo', 'tree', 'metal', 'cheese', '20', 'bedroom', 'car', 'red and white', 'fork', 'palm', 'plane', 'cows', 'glass', 'bike', 'beer', 'chinese', 'suitcase', 'sitting', 'old', 'sandwich', 'carrots', 'boat', 'stripes', 'skis', 'blonde', 'glasses', '15', 'up', 'chocolate', 'cell phone', '13', 'nike', 'tv', 'walking', 'airplane', 'sand', 'chair', 'open', 'horses', 'birthday', 'mountains', 'fruit', 'fire hydrant', 'sunglasses', 'scissors', 'ocean', 'donuts', 'cold', 'wall', 'truck', 'tie', 'coffee', 'fall', 'many', 'girl', 'clear', 'usa', 'street', 'round', 'day', 'blue and white', 'toilet', 'plaid', 'snowboard', '25', 'cooking', 'nothing', 'ski poles', 'wedding', 'wetsuit', 'knife', 'boy', 'ball', 'breakfast', 'sleeping', 'mirror', 'asian', 'africa', 'people', 'afternoon', 'toothbrush', 'plate', 'paper', 'on table', 'elephants', 'square', 'stone', 'mountain', 'window', 'dirt', 'couch', 'clock', 'city', 'no one', 'evening', '14', 'zebras', 'happy', 'chicken', 'building', 'sun', 'backpack', 'police', 'playing wii', 'daytime', 'fish', 'luggage', 'flying kite', 'plastic', 'lot', 'spoon', 'on wall', 'park', 'england', 'remote', 'picture', 'oranges', 'brushing teeth', 'birds', 'restaurant', 'refrigerator', 'pine', '50', 'watch', 'very', 'jeans', 'graffiti', 'brown and white', 'summer', 'rainy', 'background', 'talking on phone', 'sidewalk', 'safety', 'leaves', 'hay', 'concrete', 'carrot', '16', 'motorcycles', 'bicycle', 'american', 'none', 'in water', 'child', 'umbrellas', 'overcast', 'gold', 'giraffes', 'field', 'wii remote', 'playing tennis', 'microwave', 'catcher', 'resting', 'floor', 'towel', 'rocks', 'ground', '24', 'pepperoni', 'milk', 'london', 'coca cola', 'circle', 'books', '30', 'heart', 'floral', 'checkered', 'vegetables', 'racket', 'church', 'apples', 'tomato', 'surfboards', 'middle', 'in air', 'double decker', 'vase', 'small', 'rainbow', 'person', 'on', 'light', 'flower', 'box', 'bag', 'pitcher', 'closed', 'bridge', 'shadow', 'rain', 'new york', 'sink', 'morning', 'clouds', 'bowl', 'gas', 'umpire', 'rock', 'parking meter', 'keyboard', 'taking picture', 'grazing', 'sandals', 'inside', '100', 'striped', 'leather', 'bread', 'triangle', 'lights', 'drinking', 'baby', 'wire', 'reading', 'ketchup', 'blanket', 'salad', 'rope', 'china', 'sunset', 'purse', 'on left', 'computer', 'adidas', 'wilson', 'roses', 'road', '18', 'jumping', 'dessert', 'canada', 'bears', 'river', 'hot dogs', 'goggles', 'young', 'texting', 'hotel', 'glove', 'dell', 'skate park', 'shorts', 'passenger', 'on plate', 'house', 'electric', 'desk', 'book', '17', 'train station', 'soup', 'playing', 'photographer', 'oven', 'mouse', 'counter', 'cars', 'tracks', 'shirt', 'rose', 'red and yellow', 'rectangle', 'plant', 'olives', 'office', 'home', 'electricity', 'beige', 'windows', 'seagull', 'red and blue', 'guitar', 'gloves', 'star', 'spinach', 'shade', 'noon', 'large', 'delta', 'top', 'tennis ball', 'stop sign', 'smoke', 'wind', 'sky', 'sign', 'on ground', 'jacket', 'helmets', 'flying', 'branch', 'tabby', 'decoration', 'cutting board', 'bottom', 'tomatoes', 'tennis court', 'taking off', 'steam', 'stainless steel', 'sneakers', 'rug', 'lettuce', 'cup', 'basket', 'short', 'lemon', 'jet', 'indoors', 'head', 'both', 'wii controller', 'toy', 'stove', 'real', 'orange juice', 'oak', 'mustard', 'lab', 'clay', 'calm', 'blinds', 'white and blue', 'trash can', 'toilet paper', 'red and black', 'peppers', 'long', 'hot', 'hand', 'flag', 'fan', 'spring', 'ski', 'shoes', 'ponytail', 'fire', 'blue and yellow', '40', 'water skiing', 'urban', 'swimming', 'school', 'samsung', 'protection', 'pictures', 'pepsi', 'parking', 'on desk', 'market', 'flip flops', 'tea', 'talking', 'suit', 'scarf', 'posing', 'polar', 'on bed', 'landing', 'english', 'diamond', 'collar', 'at camera', 'trash', 'raining', 'parrot', 'oval', 'laying down', 'good', 'bricks', 'boats', 'away', 'shelf', 'running', 'rice', 'pole', 'meat', 'lake', 'india', 'fridge', 'fork and knife', 'clothes', 'carpet', 'unknown', 'turkey', 'straw', 'playing frisbee', 'parasailing', 'lamp', 'cross', 'color', 'chef', 'bottle', 'boots', 'yellow and blue', 'strawberries', 'skateboards', 'polo', 'orange and white', 'one way', 'headphones', 'balance', 'strawberry', 'steel', 'p', 'octagon', 'military', 'headband', 'front', 'fries', 'dogs', 'dinner', 'bicycles', '19', 'skateboarder', 'pillow', 'parking lot', 'parade', 'napkin', 'flying kites', 'cutting cake', 'cowboy', 'blender', 'bikes', '23', 'shower', 'ring', 'pug', 'polar bear', 'on sidewalk', 'mercedes', 'japan', 'hydrant', 'granite', 'german', 'dirty', 'cutting', 'collie', 'carriage', 'asia', '2 feet', 'tower', 'soda', 'reflection', 'outdoors', 'off', 'ford', 'dunkin donuts', 'duck', 'coke', 'clean', 'chain link', 'beef', 'above', 'wine glass', 'surf', 'spanish', 'pen', \"mcdonald's\", 'indian', 'hit ball', 'fake', 'dusk', 'dress', 'chopsticks', 'bracelet', 'baseball bat', 'air', '22', '21', 'wool', 'w', 'ski lift', 'siamese', 'painting', 'onions', 'men', 'lighthouse', 'italy', 'ice cream', 'honda', 'gray and white', 'grapes', 'football', 'downhill', 'candles', 'bmw', '200', 'white and red', 'watching', 'seagulls', 'red white and blue', 'ramp', 'poles', 'plants', 'owl', 'nowhere', 'no parking', 'net', 'necklace', 'lunch', 'kia', 'jump', 'ham', 'green and white', 'fishing', 'fire truck', 'ducks', 'bow', 'bacon', '27', 'warm', 'tulips', 'slow', 'on floor', 'on counter', 'north', 'marble', 'in bowl', 'fedora', 'door', 'dock', 'big ben', 'batting', 'b', '3 feet', 'van', 'toyota', 'terrier', 'tennis rackets', 'stuffed animal', 'riding', 'pot', 'pepper', 'on tracks', 'krispy kreme', 'fedex', 'dodgers', 'dining room', 'crosswalk', 'cigarette', 'big', 'basketball', 'bandana', 'bakery', 'back', 'art', 'visor', 'teddy bears', 'suitcases', 'sugar', 'stairs', 'sparrow', 'shoe', 'serve', 'public', 'paint', 'newspaper', 'monkey', 'low', 'face', 'candle', 'black and red', 'apartment', 'african', 'a', '35', '28', 'wet', 'united', 'tattoo', 'statue', 'starbucks', 'sprinkles', 'sailboat', 'parsley', 'on right', 'on grass', 'log', \"i don't know\", 'hello kitty', 'go', 'giants', 'game', 'french', 'fast', 'eagle', 'children', 'cell phones', 'broken', 'bell', 'behind', 'batter', '38', 'work', 'wooden', 'wicker', 'watermelon', 'washington', 'very tall', 'typing', 'tank top', 't', 'surfer', 'snowy', 'sausage', 'red and green', 'phones', 'obama', 'nighttime', 'moving', 'ladder', 'hats', 'german shepherd', 'full', 'foil', 'fireplace', 'cross country', 'chihuahua', 'chicago', 'catching', 'c', '55', 'yellow and red', 'working', 'white and black', 'vertical', 'vanilla', 'traveling', 'towels', 'tiger', 'stuffed animals', 'stick', 'south', 'sad', 'red sox', 'potatoes', 'playing baseball', 'pigeon', 'orioles', 'one on right', 'museum', 'medium', 'mask', 'magnets', 'little', 'horns', 'hair dryer', 'foreground', 'eggs', 'east', 'daisy', 'cumulus', 'cone', 'commercial', 'ceiling', 'cargo', 'bun', 'bucket', 'bar', 'women', 'white and brown', 'waves', 'united states', 'twin', 'tray', 'transportation', 'scooter', 'sandwiches', 'roman numerals', 'roman', 'pasta', 'navy', 'lion', 'library', 'laptops', 'high', 'harley', 'garbage', 'flags', 'cleaning', 'cats', 'butterfly', 'butter', 'bus stop', 'bulldog', 'basil', 'apron', '60', 'vest', 'tail', 'swinging', 'sweater', 'subway', 'station', 'stars', 'smile', 'shrimp', 'shallow', 'runway', 'playing game', 'pineapple', 'pickles', 'pelican', 'peace', 'party', 'outdoor', 'on street', 'microphone', 'harley davidson', 'green and yellow', 'glazed', 'europe', 'driving', 'cookies', 'ceramic', 'cart', 'calico', 'bikini', '32', '10 feet', 'yellow and black', 'wild', 'uphill', 'tusks', 'tall', 'steak', 'sony', 'smiling', 'serving', 'sauce', 'railing', 'pork', 'polka dot', 'pigeons', 'pickle', 'parked', 'on tower', 'mushrooms', 'lots', 'leash', 'handle', 'halloween', 'hair', 'grill', 'golden retriever', 'goat', 'french fries', 'forward', 'egg', 'crane', 'cleats', 'cherry', 'chandelier', 'chain', 'camouflage', 'british airways', 'barn', 'bags', '45', 'x', 'west', 'wave', 'watching tv', 'wagon', 'vases', 'straight', 'store', 'soccer ball', 'smoothie', 'selfie', 'racing', 'pizza cutter', 'pig', 'pier', 'piano', 'oil', 'nokia', 'modern', 'laying', 'kids', 'gothic', 'germany', 'garage', 'finch', 'empty', 'desert', 'deer', 'corn', 'cook', 'cones', 'circles', 'catching frisbee', 'candy', 'california', 'black and yellow', 'beard', 'barbed wire', 'army', 'all', 'adult', '33', '26', 'wires', 'water bottle', 'washington dc', 'waiting', 'toothbrushes', 'toaster oven', 'taxi', 'tag', 'suv', 'string', 'stool', 'setting', 'santa', 'power lines', 'poodle', 'pilot', 'owner', 'one on left', 'on building', 'italian', 'hungry', 'harness', 'half', 'france', 'fell', 'emirates', 'dry', 'dirt bike', 'controller', 'can', 'bull', 'british', 'bowling', 'bib', 'behind fence', 'baseball field', 'australia', 'asphalt', 'animals', '44', '29', '2013', '150', 'yellow and white', 'woods', 'volleyball', 'vegetable', 'traffic light', 'traffic', 'throwing', 'thin', 'swan', 'skull', 'skier', 'ship', 'san francisco', 'rural', 'polka dots', 'pitching', 'panda', 'out', 'nintendo', 'mozzarella', 'maple', 'man on right', 'man on left', 'm', 'lilies', 'kite flying', 'in field', 'ice', 'hospital', 'farm', 'do not enter', 'dark', 'daisies', 'curly', 'cupcake', 'cucumber', 'corner', 'clock tower', 'chips', 'cheesecake', 'chairs', 'center', 'cardboard', 'business'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfX_W-R74nJH",
        "colab_type": "text"
      },
      "source": [
        "### Build Vanilla Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_4z0GKrA4Lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initializer=tf.keras.initializers.RandomUniform(minval=-0.08, maxval=0.08)\n",
        "def img_model(img_input):\n",
        "  x = Dense(1024, activation='tanh', input_dim = 4096,kernel_initializer=initializer)(img_input)\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM29KNZPBCMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def qs_model(qs_input, num_words, embedding_dim, dropout_rate):\n",
        "  x = Embedding(len(word_index)+1, embedding_dim, input_length=SEQ_LENGTH, #len(word_index)+1\n",
        "   weights=[embedding_matrix],          trainable = False,embeddings_initializer=initializer)(qs_input)#weights=[embedding_matrix],\n",
        "  \n",
        "  x = LSTM(units=512, return_sequences=True,recurrent_dropout=0.5, input_shape= (None,embedding_dim),kernel_initializer=initializer)(x)#,dropout=0.1\n",
        "  # x = Dropout(0.5)(x)\n",
        "  x = LSTM(  units=512,recurrent_dropout=0.5, return_sequences=False,kernel_initializer=initializer)(x)#,dropout=0.2\n",
        "  # x = Dropout(0.5)(x)\n",
        "  x = Dense(1024, activation='tanh',kernel_initializer=initializer)(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP_NNUBE4uoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vanilla(num_classes, dropout_rate, num_words, embedding_dim):\n",
        "  qs_input = Input(shape=( SEQ_LENGTH ,))#train_X_seqs.shape[1]\n",
        "  img_input = Input(shape=(4096,))\n",
        "\n",
        "  CNN_model = img_model(img_input)\n",
        "  LSTM_model = qs_model(qs_input, num_words, embedding_dim, dropout_rate)\n",
        "\n",
        "  x = Multiply()([CNN_model, LSTM_model])\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Dense(1000, activation='tanh',kernel_initializer=initializer)(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  output = Dense(num_classes, activation='softmax',kernel_initializer=initializer)(x)\n",
        "  \n",
        "  model = Model(inputs= [qs_input, img_input], outputs= output)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH-EWGFhw8lo",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv5Srzj67DQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_size=215359\n",
        "val_size=121512\n",
        "#215359\n",
        "#121512\n",
        "\n",
        "train_X_seqs_final=train_X_seqs[0:train_size]\n",
        "train_X_ims_final=train_X_ims[0:train_size]\n",
        "train_Y_final=train_Y[0:train_size]\n",
        "val_X_seqs_final=val_X_seqs[0:val_size]\n",
        "val_X_ims_final=val_X_ims[0:val_size]\n",
        "val_Y_final=val_Y[0:val_size]\n",
        "\n",
        "\n",
        "n_epochs=50\n",
        "b_size=500\n",
        "steps_per_epoch = int( np.ceil(train_X_seqs_final.shape[0] / b_size) )\n",
        "learning_rate_decay_every=50 #50000 #300\n",
        "in_learning_rate= 3e-4 #1e-3#3e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJRSrSb9SdAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X_seqs=None\n",
        "train_X_ims=None\n",
        "train_Y=None\n",
        "val_X_seqs=None\n",
        "val_X_ims=None\n",
        "val_Y=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7akQN8iXipQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X_seqs_final=None\n",
        "train_X_ims_final=None\n",
        "train_Y_final=None\n",
        "val_X_seqs_final=None\n",
        "val_X_ims_final=None\n",
        "val_Y_final=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP5M0OU1XV0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_X_ims=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq1S0ZY4KwKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=in_learning_rate,#1e-3\n",
        "    decay_steps=learning_rate_decay_every,\n",
        "    # decay_rate=math.exp(math.log(0.1)/learning_rate_decay_every/steps_per_epoch))#0.9\n",
        "    decay_rate = 0.99997592083)\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=lr_schedule)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM2sdG2KC-HX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt=tf.keras.optimizers.RMSprop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bEjAHs_NzRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt=tf.keras.optimizers.RMSprop(lr=0.0004)#, rho=0.99, epsilon=1e-08, decay=300\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr7Gy0FzZJPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt=tf.keras.optimizers.Adadelta(lr=1)#lr=0.00004, rho=0.99, epsilon=1e-08, decay=300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4_LGW6m9DrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normal_loss(y_true, y_pred):\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(y_true,1), logits=y_pred)\n",
        "    return(tf.reduce_sum(loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJqRDTTHw_eO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Train():\n",
        "  checkpoint = ModelCheckpoint('model.h5', save_best_only=True)\n",
        "\n",
        "  earlystop_callback = EarlyStopping(\n",
        "  monitor='val_loss', min_delta=0.0001,\n",
        "  patience=10)\n",
        "\n",
        "  model = vanilla(NUM_OF_CLASSES, DROPOUT_RATE, len(word_index)+1, EMBEDDING_DIM)\n",
        "  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])#'categorical_crossentropy'\n",
        "  model.summary()\n",
        "  history = model.fit([train_X_seqs_final,train_X_ims_final] ,\n",
        "                      train_Y_final, \n",
        "                      epochs = n_epochs, \n",
        "                      batch_size = b_size#,\n",
        "                      #callbacks=[earlystop_callback], \n",
        "                      # callbacks = [checkpoint],\n",
        "                      , validation_data=([val_X_seqs_final,val_X_ims_final], val_Y_final) \n",
        "                     , shuffle=True\n",
        "                      )#shuffle=True\n",
        "  return history, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asRxEOY-_qfT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(123)\n",
        "python_random.seed(123)\n",
        "tf.random.set_seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2sPcer6Biey",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "f9e3ab92-72ac-4a42-cddf-60313f577691"
      },
      "source": [
        "\n",
        "tf.keras.backend.clear_session()\n",
        "history, model = Train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 26)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 26, 300)      3957600     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 26, 512)      1665024     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 4096)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 512)          2099200     lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1024)         4195328     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1024)         525312      lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 1024)         0           dense[0][0]                      \n",
            "                                                                 dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 1024)         0           multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1000)         1025000     dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1000)         0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1000)         1001000     dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 14,468,464\n",
            "Trainable params: 10,510,864\n",
            "Non-trainable params: 3,957,600\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            " 18/431 [>.............................] - ETA: 47:46 - loss: 5.2844 - accuracy: 0.2177"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-f55e3b103e36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-5002eb5f9ec5>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                       \u001b[0;31m# callbacks = [checkpoint],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                       \u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_X_seqs_final\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_X_ims_final\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_Y_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                      \u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                       )#shuffle=True\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l7qqvrlOKGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_accuracy_loss(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TpkEEXm1BYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "05327f27-e4e9-4060-e198-72c9ab085fd9"
      },
      "source": [
        "def Evaluation(model):\n",
        "  results = model.evaluate([val_X_seqs_final,val_X_ims_final], val_Y_final, batch_size=300)\n",
        "  print('val loss, val acc:', results)\n",
        "Evaluation(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "406/406 [==============================] - 11s 26ms/step - loss: 3.2440 - accuracy: 0.4090\n",
            "val loss, val acc: [3.2440288066864014, 0.4090377986431122]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_CMHFB6WIwy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "fb9abeeb-8d85-43a2-96a9-02752269b243"
      },
      "source": [
        "model.save(BASE_PATH_parssoft+'model_persian_expid_13.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-fb0d0039d564>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_PATH_parssoft\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'model_persian_expid_13.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou-AUdbiWHgz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e5661d15-30df-4109-f58a-d410ccebcde8"
      },
      "source": [
        "result = []\n",
        "\n",
        "p = model.predict([val_X_seqs_final,val_X_ims_final],batch_size=256)\n",
        "\n",
        "for q in range(len(val_q_ids)):\n",
        "  ans=indextoanswer[p[q].argmax(axis=-1)]\n",
        "  q_id=val_q_ids[q]\n",
        "  result.append({u'answer': ans, u'question_id': q_id})\n",
        "\n",
        "  # if q%50000==0 :print(q)\n",
        "\n",
        "print('Saving result...')\n",
        "my_list = list(result)\n",
        "dd = json.dump(my_list,open(BASE_PATH_parssoft2+'result.json','w'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving result...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZwLZgluivjF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "f59bab8c-ee00-4848-dec0-bdc9a928ab32"
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "import sys\n",
        "dataDir = '/content/drive/My Drive/parssoftco_PVQA2'\n",
        "sys.path.insert(0, '/content/drive/My Drive/parssoftco_PVQA/')\n",
        "from vqaTools.vqa import VQA\n",
        "from vqaEvaluation.vqaEval import VQAEval\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io as io\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "\n",
        "# set up file names and paths\n",
        "versionType ='' # this should be '' when using VQA v2.0 dataset\n",
        "taskType    ='OpenEnded' # 'OpenEnded' only for v2.0. 'OpenEnded' or 'MultipleChoice' for v1.0\n",
        "dataType    ='mscoco'  # 'mscoco' only for v1.0. 'mscoco' for real and 'abstract_v002' for abstract for v1.0. \n",
        "dataSubType ='val2014'\n",
        "annFile     ='%s/%s%s_%s_annotations.json'%(dataDir, versionType, dataType, dataSubType)\n",
        "quesFile    ='%s/google-val_questions.json'%(dataDir)\n",
        "imgDir      ='%s/Images/%s/%s/' %(dataDir, dataType, dataSubType)\n",
        "fileTypes   = ['results', 'accuracy', 'evalQA', 'evalQuesType', 'evalAnsType']\n",
        "\n",
        "# An example result json file has been provided in './Results' folder.  \n",
        "\n",
        "[resFile, accuracyFile, evalQAFile, evalQuesTypeFile, evalAnsTypeFile] =['%s/result.json'%(dataDir) for fileType in fileTypes]\n",
        "# create vqa object and vqaRes object\n",
        "vqa = VQA(annFile, quesFile)\n",
        "vqaRes = vqa.loadRes(resFile, quesFile)\n",
        "\n",
        "# create vqaEval object by taking vqa and vqaRes\n",
        "vqaEval = VQAEval(vqa, vqaRes, n=2)   #n is precision of accuracy (number of places after decimal), default is 2\n",
        "\n",
        "# evaluate results\n",
        "\"\"\"\n",
        "If you have a list of question ids on which you would like to evaluate your results, pass it as a list to below function\n",
        "By default it uses all the question ids in annotation file\n",
        "\"\"\"\n",
        "vqaEval.evaluate() \n",
        "\n",
        "# print accuracies\n",
        "print (\"\\n\")\n",
        "print (\"Overall Accuracy is: %.02f\\n\" %(vqaEval.accuracy['overall']))\n",
        "\n",
        "print (\"Per Answer Type Accuracy is the following:\")\n",
        "for ansType in vqaEval.accuracy['perAnswerType']:\n",
        "\tprint (\"%s : %.02f\" %(ansType, vqaEval.accuracy['perAnswerType'][ansType]))\n",
        "print (\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading VQA annotations and questions into memory...\n",
            "0:00:06.490218\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...     \n",
            "DONE (t=0.31s)\n",
            "creating index...\n",
            "index created!\n",
            "computing accuracy\n",
            "Finshed Percent: [####################] 99% Done computing accuracy\n",
            "\n",
            "\n",
            "Overall Accuracy is: 51.21\n",
            "\n",
            "Per Answer Type Accuracy is the following:\n",
            "other : 35.55\n",
            "yes/no : 78.12\n",
            "number : 33.43\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fFWWXTgptzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, TFAutoModel\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "model = TFAutoModel.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "\n",
        "text = \"ما در هوشواره معتقدیم با انتقال صحیح دانش و آگاهی، همه افراد می‌توانند از ابزارهای هوشمند استفاده کنند. شعار ما هوش مصنوعی برای همه است.\"\n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gu8cOXbpy0C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1b85911-e0f1-440d-fc90-d7e0a0ef7f01"
      },
      "source": [
        "def f2():\n",
        "  return f1()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2u6mtdYqIQR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f4befd3-f081-4da9-c9fc-bbb9d7978801"
      },
      "source": [
        "print(f2())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UgYacz5IKR5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "556c66f6-dce0-4db1-fb95-70ee04cbd5d0"
      },
      "source": [
        "a=[1,2,3]\n",
        "a[:2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HbfgCxpGUN2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "680df40e-34ee-418b-c287-058720b6e571"
      },
      "source": [
        "!python /content/drive/My\\ Drive/parssoftco_PVQA/vqaEvalDemo.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading VQA annotations and questions into memory...\n",
            "0:00:01.991607\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...     \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/My Drive/parssoftco_PVQA/vqaEvalDemo.py\", line 29, in <module>\n",
            "    vqaRes = vqa.loadRes(resFile, quesFile)\n",
            "  File \"/content/drive/My Drive/parssoftco_PVQA/vqaTools/vqa.py\", line 159, in loadRes\n",
            "    assert type(anns) == list, 'results is not an array of objects'\n",
            "AssertionError: results is not an array of objects\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1sex9ZymQJ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "9312777b-93cd-4475-e84f-6f475491e9ef"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model.save(BASE_PATH_parssoft+ 'model_alldata_30ep_eng_rmslrdecay.h5', overwrite=True)  # creates a HDF5 file 'my_model.h5'\n",
        "del model  # deletes the existing model\n",
        "\n",
        "# returns a compiled model\n",
        "# identical to the previous one\n",
        "# model = load_model('my_model.h5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-220-37f65479cbe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_PATH_parssoft\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'model_alldata_30ep_eng_rmslrdecay.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# creates a HDF5 file 'my_model.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m  \u001b[0;31m# deletes the existing model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \"\"\"\n\u001b[1;32m   1051\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1052\u001b[0;31m                     signatures, options)\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    133\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    134\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 135\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    136\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    123\u001b[0m     if (include_optimizer and model.optimizer and\n\u001b[1;32m    124\u001b[0m         not isinstance(model.optimizer, optimizers.TFOptimizer)):\n\u001b[0;32m--> 125\u001b[0;31m       \u001b[0msave_optimizer_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_optimizer_weights_to_hdf5_group\u001b[0;34m(hdf5_group, optimizer)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m       param_dset = weights_group.create_dataset(\n\u001b[0;32m--> 593\u001b[0;31m           name, val.shape, dtype=val.dtype)\n\u001b[0m\u001b[1;32m    594\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, obj)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHLObject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSoftLink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.link\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unable to create link (name already exists)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MzayYUlwLWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtaA37jxEGaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_accuracy_loss(history):\n",
        "  acc      = history.history[     'accuracy' ]\n",
        "  val_acc  = history.history[ 'val_accuracy' ]\n",
        "  loss     = history.history[    'loss' ]\n",
        "  val_loss = history.history['val_loss' ]\n",
        "\n",
        "  epochs   = range(len(acc))\n",
        "\n",
        "  plt.plot  ( epochs,     acc, label='train_acc' )\n",
        "  plt.plot  ( epochs, val_acc, label='val_acc' )\n",
        "  plt.title ('Training and validation accuracy')\n",
        "  plt.legend()\n",
        "  plt.savefig(BASE_PATH + 'Training and validation accuracy.jpg')\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot  ( epochs,     loss, label='train_loss' )\n",
        "  plt.plot  ( epochs, val_loss, label='val_loss' )\n",
        "  plt.title ('Training and validation loss'   )\n",
        "  plt.legend()\n",
        "  plt.savefig(BASE_PATH + 'Training and validation loss.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4OteN2u22vj",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQtv4GmfkUNd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "eab172c6-8e3f-45cd-8336-97efbd7a00d7"
      },
      "source": [
        "# Download the VQA Questions from http://www.visualqa.org/download.html\n",
        "import json\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "\n",
        "'''\n",
        "Put the VQA data into single json file, where [[Question_id, Image_id, Question, multipleChoice_answer, Answer] ... ]\n",
        "'''\n",
        "\n",
        "train = []\n",
        "test = []\n",
        "imdir='%s/COCO_%s_%012d.jpg'\n",
        "\n",
        "\n",
        "print('Loading annotations and questions...')\n",
        "train_anno = json.load(open(ANNOTATION_TRAIN_PATH, 'r'))\n",
        "val_anno = json.load(open(ANNOTATION_VAL_PATH, 'r'))\n",
        "\n",
        "train_ques = json.load(open(QUESTION_TRAIN_PATH, 'r'))\n",
        "val_ques = json.load(open(QUESTION_VAL_PATH, 'r'))\n",
        "\n",
        "subtype = 'train2014'\n",
        "for i in range(len(train_anno['annotations'])):\n",
        "    ans = train_anno['annotations'][i]['multiple_choice_answer']\n",
        "    question_id = train_anno['annotations'][i]['question_id']\n",
        "    image_path = imdir%(subtype, subtype, train_anno['annotations'][i]['image_id'])\n",
        "\n",
        "    question = train_ques['questions'][i]['question']\n",
        "    mc_ans = train_ques['questions'][i]['question']\n",
        "\n",
        "    train.append({'ques_id': question_id, 'img_path': image_path, 'question': question, 'MC_ans': mc_ans, 'ans': ans})\n",
        "\n",
        "subtype = 'val2014'\n",
        "for i in range(len(val_anno['annotations'])):\n",
        "    ans = val_anno['annotations'][i]['multiple_choice_answer']\n",
        "    question_id = val_anno['annotations'][i]['question_id']\n",
        "    image_path = imdir%(subtype, subtype, val_anno['annotations'][i]['image_id'])\n",
        "\n",
        "    question = val_ques['questions'][i]['question']\n",
        "    mc_ans = val_ques['questions'][i]['question']\n",
        "\n",
        "    test.append({'ques_id': question_id, 'img_path': image_path, 'question': question, 'MC_ans': mc_ans, 'ans': ans})\n",
        "\n",
        "\n",
        "print('Training sample %d, Testing sample %d...' %(len(train), len(test)))\n",
        "\n",
        "json.dump(train, open('vqa_raw_train.json', 'w'))\n",
        "json.dump(test, open('vqa_raw_test.json', 'w'))\n",
        "\n",
        "\n",
        "# !cp /content/vqa_raw_train.json /content/drive/My\\ Drive/parssoftco_PVQA/vqa_raw_train.json\n",
        "# !cp /content/vqa_raw_test.json /content/drive/My\\ Drive/parssoftco_PVQA/vqa_raw_test.json\n",
        "\n",
        "\n",
        "def get_top_answers(imgs, params):\n",
        "  counts = {}\n",
        "  for img in imgs:\n",
        "    ans = img['ans']\n",
        "    counts[ans] = counts.get(ans, 0) + 1\n",
        "  cw = sorted([(count,w) for w,count in counts.items()], reverse=True)\n",
        "  vocab = []\n",
        "  for i in range(params):\n",
        "      vocab.append(cw[i][1])\n",
        "  return vocab[:params]\n",
        "\n",
        "\n",
        "def filter_question(imgs, atoi):\n",
        "    new_imgs = []\n",
        "    for i, img in enumerate(imgs):\n",
        "        if atoi.get(img['ans'],len(atoi)+1) != len(atoi)+1:\n",
        "            new_imgs.append(img)\n",
        "\n",
        "    print('question number reduce from %d to %d '%(len(imgs), len(new_imgs)))\n",
        "    return new_imgs\n",
        "imgs_train = json.load(open('vqa_raw_train.json', 'r'))\n",
        "imgs_test = json.load(open('vqa_raw_test.json', 'r'))\n",
        "\n",
        "# get top answers\n",
        "top_ans = get_top_answers(imgs_train, 999)\n",
        "atoi = {w:i+1 for i,w in enumerate(top_ans)}\n",
        "itoa = {i+1:w for i,w in enumerate(top_ans)}\n",
        "\n",
        "# filter question, which isn't in the top answers.\n",
        "imgs_train = filter_question(imgs_train, atoi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading annotations and questions...\n",
            "Training sample 248349, Testing sample 121512...\n",
            "question number reduce from 248349 to 215359 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdUAuZOf8XoS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fb0f5e15-2dc2-43c3-f1a5-be31cd3ee5c8"
      },
      "source": [
        "q = json.load(open('vqa_raw_train.json', 'r'))\n",
        "print(q[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'ques_id': 4870250, 'img_path': 'train2014/COCO_train2014_000000487025.jpg', 'question': 'What shape is the bench seat?', 'MC_ans': 'What shape is the bench seat?', 'ans': 'curved'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKyHgjE0DYsd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3032421a-f44e-482f-f8c4-f226d3eea6d0"
      },
      "source": [
        "imgs_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'MC_ans': 'Is there a shadow?',\n",
              " 'ans': 'yes',\n",
              " 'img_path': 'train2014/COCO_train2014_000000487025.jpg',\n",
              " 'ques_id': 4870251,\n",
              " 'question': 'Is there a shadow?'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PSKiHFT8lT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imgs_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4Ehj85B_wi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhdLAVyp83M4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "65725d3e-9aaa-4e56-80fb-78e4e6243bab"
      },
      "source": [
        "# img_data = h5py.File(data_img)\n",
        "ques_data = h5py.File('data_prepro.h5')\n",
        "\n",
        "# img_data = np.array(img_data['images_train'])\n",
        "\n",
        "# NOTE should've consturcted one-hots using exhausitve list of answers, cause some answers may not be in dataset\n",
        "# To temporarily rectify this, all those answer indices is set to 1 in validation set\n",
        "train_y = to_categorical(ques_data['answers'])[:, :]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StgYdEhCBD3U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4e1e4fa-7acf-4464-dd1a-bf671b7d7f5c"
      },
      "source": [
        "train_y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(215359, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5Vdvp8lCkNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E922I1NCIav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e=[]\n",
        "for s in ques_data['answers']:\n",
        "  e.append(s)\n",
        "set(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oghMBGvBOl9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c5881925-8de4-4180-f2e3-a040bc6ada26"
      },
      "source": [
        "e=[]\n",
        "for s in ques_data['answers']:\n",
        "  e.append(s)\n",
        "set(e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 10,\n",
              " 11,\n",
              " 12,\n",
              " 13,\n",
              " 14,\n",
              " 15,\n",
              " 16,\n",
              " 17,\n",
              " 18,\n",
              " 19,\n",
              " 20,\n",
              " 21,\n",
              " 22,\n",
              " 23,\n",
              " 24,\n",
              " 25,\n",
              " 26,\n",
              " 27,\n",
              " 28,\n",
              " 29,\n",
              " 30,\n",
              " 31,\n",
              " 32,\n",
              " 33,\n",
              " 34,\n",
              " 35,\n",
              " 36,\n",
              " 37,\n",
              " 38,\n",
              " 39,\n",
              " 40,\n",
              " 41,\n",
              " 42,\n",
              " 43,\n",
              " 44,\n",
              " 45,\n",
              " 46,\n",
              " 47,\n",
              " 48,\n",
              " 49,\n",
              " 50,\n",
              " 51,\n",
              " 52,\n",
              " 53,\n",
              " 54,\n",
              " 55,\n",
              " 56,\n",
              " 57,\n",
              " 58,\n",
              " 59,\n",
              " 60,\n",
              " 61,\n",
              " 62,\n",
              " 63,\n",
              " 64,\n",
              " 65,\n",
              " 66,\n",
              " 67,\n",
              " 68,\n",
              " 69,\n",
              " 70,\n",
              " 71,\n",
              " 72,\n",
              " 73,\n",
              " 74,\n",
              " 75,\n",
              " 76,\n",
              " 77,\n",
              " 78,\n",
              " 79,\n",
              " 80,\n",
              " 81,\n",
              " 82,\n",
              " 83,\n",
              " 84,\n",
              " 85,\n",
              " 86,\n",
              " 87,\n",
              " 88,\n",
              " 89,\n",
              " 90,\n",
              " 91,\n",
              " 92,\n",
              " 93,\n",
              " 94,\n",
              " 95,\n",
              " 96,\n",
              " 97,\n",
              " 98,\n",
              " 99,\n",
              " 100,\n",
              " 101,\n",
              " 102,\n",
              " 103,\n",
              " 104,\n",
              " 105,\n",
              " 106,\n",
              " 107,\n",
              " 108,\n",
              " 109,\n",
              " 110,\n",
              " 111,\n",
              " 112,\n",
              " 113,\n",
              " 114,\n",
              " 115,\n",
              " 116,\n",
              " 117,\n",
              " 118,\n",
              " 119,\n",
              " 120,\n",
              " 121,\n",
              " 122,\n",
              " 123,\n",
              " 124,\n",
              " 125,\n",
              " 126,\n",
              " 127,\n",
              " 128,\n",
              " 129,\n",
              " 130,\n",
              " 131,\n",
              " 132,\n",
              " 133,\n",
              " 134,\n",
              " 135,\n",
              " 136,\n",
              " 137,\n",
              " 138,\n",
              " 139,\n",
              " 140,\n",
              " 141,\n",
              " 142,\n",
              " 143,\n",
              " 144,\n",
              " 145,\n",
              " 146,\n",
              " 147,\n",
              " 148,\n",
              " 149,\n",
              " 150,\n",
              " 151,\n",
              " 152,\n",
              " 153,\n",
              " 154,\n",
              " 155,\n",
              " 156,\n",
              " 157,\n",
              " 158,\n",
              " 159,\n",
              " 160,\n",
              " 161,\n",
              " 162,\n",
              " 163,\n",
              " 164,\n",
              " 165,\n",
              " 166,\n",
              " 167,\n",
              " 168,\n",
              " 169,\n",
              " 170,\n",
              " 171,\n",
              " 172,\n",
              " 173,\n",
              " 174,\n",
              " 175,\n",
              " 176,\n",
              " 177,\n",
              " 178,\n",
              " 179,\n",
              " 180,\n",
              " 181,\n",
              " 182,\n",
              " 183,\n",
              " 184,\n",
              " 185,\n",
              " 186,\n",
              " 187,\n",
              " 188,\n",
              " 189,\n",
              " 190,\n",
              " 191,\n",
              " 192,\n",
              " 193,\n",
              " 194,\n",
              " 195,\n",
              " 196,\n",
              " 197,\n",
              " 198,\n",
              " 199,\n",
              " 200,\n",
              " 201,\n",
              " 202,\n",
              " 203,\n",
              " 204,\n",
              " 205,\n",
              " 206,\n",
              " 207,\n",
              " 208,\n",
              " 209,\n",
              " 210,\n",
              " 211,\n",
              " 212,\n",
              " 213,\n",
              " 214,\n",
              " 215,\n",
              " 216,\n",
              " 217,\n",
              " 218,\n",
              " 219,\n",
              " 220,\n",
              " 221,\n",
              " 222,\n",
              " 223,\n",
              " 224,\n",
              " 225,\n",
              " 226,\n",
              " 227,\n",
              " 228,\n",
              " 229,\n",
              " 230,\n",
              " 231,\n",
              " 232,\n",
              " 233,\n",
              " 234,\n",
              " 235,\n",
              " 236,\n",
              " 237,\n",
              " 238,\n",
              " 239,\n",
              " 240,\n",
              " 241,\n",
              " 242,\n",
              " 243,\n",
              " 244,\n",
              " 245,\n",
              " 246,\n",
              " 247,\n",
              " 248,\n",
              " 249,\n",
              " 250,\n",
              " 251,\n",
              " 252,\n",
              " 253,\n",
              " 254,\n",
              " 255,\n",
              " 256,\n",
              " 257,\n",
              " 258,\n",
              " 259,\n",
              " 260,\n",
              " 261,\n",
              " 262,\n",
              " 263,\n",
              " 264,\n",
              " 265,\n",
              " 266,\n",
              " 267,\n",
              " 268,\n",
              " 269,\n",
              " 270,\n",
              " 271,\n",
              " 272,\n",
              " 273,\n",
              " 274,\n",
              " 275,\n",
              " 276,\n",
              " 277,\n",
              " 278,\n",
              " 279,\n",
              " 280,\n",
              " 281,\n",
              " 282,\n",
              " 283,\n",
              " 284,\n",
              " 285,\n",
              " 286,\n",
              " 287,\n",
              " 288,\n",
              " 289,\n",
              " 290,\n",
              " 291,\n",
              " 292,\n",
              " 293,\n",
              " 294,\n",
              " 295,\n",
              " 296,\n",
              " 297,\n",
              " 298,\n",
              " 299,\n",
              " 300,\n",
              " 301,\n",
              " 302,\n",
              " 303,\n",
              " 304,\n",
              " 305,\n",
              " 306,\n",
              " 307,\n",
              " 308,\n",
              " 309,\n",
              " 310,\n",
              " 311,\n",
              " 312,\n",
              " 313,\n",
              " 314,\n",
              " 315,\n",
              " 316,\n",
              " 317,\n",
              " 318,\n",
              " 319,\n",
              " 320,\n",
              " 321,\n",
              " 322,\n",
              " 323,\n",
              " 324,\n",
              " 325,\n",
              " 326,\n",
              " 327,\n",
              " 328,\n",
              " 329,\n",
              " 330,\n",
              " 331,\n",
              " 332,\n",
              " 333,\n",
              " 334,\n",
              " 335,\n",
              " 336,\n",
              " 337,\n",
              " 338,\n",
              " 339,\n",
              " 340,\n",
              " 341,\n",
              " 342,\n",
              " 343,\n",
              " 344,\n",
              " 345,\n",
              " 346,\n",
              " 347,\n",
              " 348,\n",
              " 349,\n",
              " 350,\n",
              " 351,\n",
              " 352,\n",
              " 353,\n",
              " 354,\n",
              " 355,\n",
              " 356,\n",
              " 357,\n",
              " 358,\n",
              " 359,\n",
              " 360,\n",
              " 361,\n",
              " 362,\n",
              " 363,\n",
              " 364,\n",
              " 365,\n",
              " 366,\n",
              " 367,\n",
              " 368,\n",
              " 369,\n",
              " 370,\n",
              " 371,\n",
              " 372,\n",
              " 373,\n",
              " 374,\n",
              " 375,\n",
              " 376,\n",
              " 377,\n",
              " 378,\n",
              " 379,\n",
              " 380,\n",
              " 381,\n",
              " 382,\n",
              " 383,\n",
              " 384,\n",
              " 385,\n",
              " 386,\n",
              " 387,\n",
              " 388,\n",
              " 389,\n",
              " 390,\n",
              " 391,\n",
              " 392,\n",
              " 393,\n",
              " 394,\n",
              " 395,\n",
              " 396,\n",
              " 397,\n",
              " 398,\n",
              " 399,\n",
              " 400,\n",
              " 401,\n",
              " 402,\n",
              " 403,\n",
              " 404,\n",
              " 405,\n",
              " 406,\n",
              " 407,\n",
              " 408,\n",
              " 409,\n",
              " 410,\n",
              " 411,\n",
              " 412,\n",
              " 413,\n",
              " 414,\n",
              " 415,\n",
              " 416,\n",
              " 417,\n",
              " 418,\n",
              " 419,\n",
              " 420,\n",
              " 421,\n",
              " 422,\n",
              " 423,\n",
              " 424,\n",
              " 425,\n",
              " 426,\n",
              " 427,\n",
              " 428,\n",
              " 429,\n",
              " 430,\n",
              " 431,\n",
              " 432,\n",
              " 433,\n",
              " 434,\n",
              " 435,\n",
              " 436,\n",
              " 437,\n",
              " 438,\n",
              " 439,\n",
              " 440,\n",
              " 441,\n",
              " 442,\n",
              " 443,\n",
              " 444,\n",
              " 445,\n",
              " 446,\n",
              " 447,\n",
              " 448,\n",
              " 449,\n",
              " 450,\n",
              " 451,\n",
              " 452,\n",
              " 453,\n",
              " 454,\n",
              " 455,\n",
              " 456,\n",
              " 457,\n",
              " 458,\n",
              " 459,\n",
              " 460,\n",
              " 461,\n",
              " 462,\n",
              " 463,\n",
              " 464,\n",
              " 465,\n",
              " 466,\n",
              " 467,\n",
              " 468,\n",
              " 469,\n",
              " 470,\n",
              " 471,\n",
              " 472,\n",
              " 473,\n",
              " 474,\n",
              " 475,\n",
              " 476,\n",
              " 477,\n",
              " 478,\n",
              " 479,\n",
              " 480,\n",
              " 481,\n",
              " 482,\n",
              " 483,\n",
              " 484,\n",
              " 485,\n",
              " 486,\n",
              " 487,\n",
              " 488,\n",
              " 489,\n",
              " 490,\n",
              " 491,\n",
              " 492,\n",
              " 493,\n",
              " 494,\n",
              " 495,\n",
              " 496,\n",
              " 497,\n",
              " 498,\n",
              " 499,\n",
              " 500,\n",
              " 501,\n",
              " 502,\n",
              " 503,\n",
              " 504,\n",
              " 505,\n",
              " 506,\n",
              " 507,\n",
              " 508,\n",
              " 509,\n",
              " 510,\n",
              " 511,\n",
              " 512,\n",
              " 513,\n",
              " 514,\n",
              " 515,\n",
              " 516,\n",
              " 517,\n",
              " 518,\n",
              " 519,\n",
              " 520,\n",
              " 521,\n",
              " 522,\n",
              " 523,\n",
              " 524,\n",
              " 525,\n",
              " 526,\n",
              " 527,\n",
              " 528,\n",
              " 529,\n",
              " 530,\n",
              " 531,\n",
              " 532,\n",
              " 533,\n",
              " 534,\n",
              " 535,\n",
              " 536,\n",
              " 537,\n",
              " 538,\n",
              " 539,\n",
              " 540,\n",
              " 541,\n",
              " 542,\n",
              " 543,\n",
              " 544,\n",
              " 545,\n",
              " 546,\n",
              " 547,\n",
              " 548,\n",
              " 549,\n",
              " 550,\n",
              " 551,\n",
              " 552,\n",
              " 553,\n",
              " 554,\n",
              " 555,\n",
              " 556,\n",
              " 557,\n",
              " 558,\n",
              " 559,\n",
              " 560,\n",
              " 561,\n",
              " 562,\n",
              " 563,\n",
              " 564,\n",
              " 565,\n",
              " 566,\n",
              " 567,\n",
              " 568,\n",
              " 569,\n",
              " 570,\n",
              " 571,\n",
              " 572,\n",
              " 573,\n",
              " 574,\n",
              " 575,\n",
              " 576,\n",
              " 577,\n",
              " 578,\n",
              " 579,\n",
              " 580,\n",
              " 581,\n",
              " 582,\n",
              " 583,\n",
              " 584,\n",
              " 585,\n",
              " 586,\n",
              " 587,\n",
              " 588,\n",
              " 589,\n",
              " 590,\n",
              " 591,\n",
              " 592,\n",
              " 593,\n",
              " 594,\n",
              " 595,\n",
              " 596,\n",
              " 597,\n",
              " 598,\n",
              " 599,\n",
              " 600,\n",
              " 601,\n",
              " 602,\n",
              " 603,\n",
              " 604,\n",
              " 605,\n",
              " 606,\n",
              " 607,\n",
              " 608,\n",
              " 609,\n",
              " 610,\n",
              " 611,\n",
              " 612,\n",
              " 613,\n",
              " 614,\n",
              " 615,\n",
              " 616,\n",
              " 617,\n",
              " 618,\n",
              " 619,\n",
              " 620,\n",
              " 621,\n",
              " 622,\n",
              " 623,\n",
              " 624,\n",
              " 625,\n",
              " 626,\n",
              " 627,\n",
              " 628,\n",
              " 629,\n",
              " 630,\n",
              " 631,\n",
              " 632,\n",
              " 633,\n",
              " 634,\n",
              " 635,\n",
              " 636,\n",
              " 637,\n",
              " 638,\n",
              " 639,\n",
              " 640,\n",
              " 641,\n",
              " 642,\n",
              " 643,\n",
              " 644,\n",
              " 645,\n",
              " 646,\n",
              " 647,\n",
              " 648,\n",
              " 649,\n",
              " 650,\n",
              " 651,\n",
              " 652,\n",
              " 653,\n",
              " 654,\n",
              " 655,\n",
              " 656,\n",
              " 657,\n",
              " 658,\n",
              " 659,\n",
              " 660,\n",
              " 661,\n",
              " 662,\n",
              " 663,\n",
              " 664,\n",
              " 665,\n",
              " 666,\n",
              " 667,\n",
              " 668,\n",
              " 669,\n",
              " 670,\n",
              " 671,\n",
              " 672,\n",
              " 673,\n",
              " 674,\n",
              " 675,\n",
              " 676,\n",
              " 677,\n",
              " 678,\n",
              " 679,\n",
              " 680,\n",
              " 681,\n",
              " 682,\n",
              " 683,\n",
              " 684,\n",
              " 685,\n",
              " 686,\n",
              " 687,\n",
              " 688,\n",
              " 689,\n",
              " 690,\n",
              " 691,\n",
              " 692,\n",
              " 693,\n",
              " 694,\n",
              " 695,\n",
              " 696,\n",
              " 697,\n",
              " 698,\n",
              " 699,\n",
              " 700,\n",
              " 701,\n",
              " 702,\n",
              " 703,\n",
              " 704,\n",
              " 705,\n",
              " 706,\n",
              " 707,\n",
              " 708,\n",
              " 709,\n",
              " 710,\n",
              " 711,\n",
              " 712,\n",
              " 713,\n",
              " 714,\n",
              " 715,\n",
              " 716,\n",
              " 717,\n",
              " 718,\n",
              " 719,\n",
              " 720,\n",
              " 721,\n",
              " 722,\n",
              " 723,\n",
              " 724,\n",
              " 725,\n",
              " 726,\n",
              " 727,\n",
              " 728,\n",
              " 729,\n",
              " 730,\n",
              " 731,\n",
              " 732,\n",
              " 733,\n",
              " 734,\n",
              " 735,\n",
              " 736,\n",
              " 737,\n",
              " 738,\n",
              " 739,\n",
              " 740,\n",
              " 741,\n",
              " 742,\n",
              " 743,\n",
              " 744,\n",
              " 745,\n",
              " 746,\n",
              " 747,\n",
              " 748,\n",
              " 749,\n",
              " 750,\n",
              " 751,\n",
              " 752,\n",
              " 753,\n",
              " 754,\n",
              " 755,\n",
              " 756,\n",
              " 757,\n",
              " 758,\n",
              " 759,\n",
              " 760,\n",
              " 761,\n",
              " 762,\n",
              " 763,\n",
              " 764,\n",
              " 765,\n",
              " 766,\n",
              " 767,\n",
              " 768,\n",
              " 769,\n",
              " 770,\n",
              " 771,\n",
              " 772,\n",
              " 773,\n",
              " 774,\n",
              " 775,\n",
              " 776,\n",
              " 777,\n",
              " 778,\n",
              " 779,\n",
              " 780,\n",
              " 781,\n",
              " 782,\n",
              " 783,\n",
              " 784,\n",
              " 785,\n",
              " 786,\n",
              " 787,\n",
              " 788,\n",
              " 789,\n",
              " 790,\n",
              " 791,\n",
              " 792,\n",
              " 793,\n",
              " 794,\n",
              " 795,\n",
              " 796,\n",
              " 797,\n",
              " 798,\n",
              " 799,\n",
              " 800,\n",
              " 801,\n",
              " 802,\n",
              " 803,\n",
              " 804,\n",
              " 805,\n",
              " 806,\n",
              " 807,\n",
              " 808,\n",
              " 809,\n",
              " 810,\n",
              " 811,\n",
              " 812,\n",
              " 813,\n",
              " 814,\n",
              " 815,\n",
              " 816,\n",
              " 817,\n",
              " 818,\n",
              " 819,\n",
              " 820,\n",
              " 821,\n",
              " 822,\n",
              " 823,\n",
              " 824,\n",
              " 825,\n",
              " 826,\n",
              " 827,\n",
              " 828,\n",
              " 829,\n",
              " 830,\n",
              " 831,\n",
              " 832,\n",
              " 833,\n",
              " 834,\n",
              " 835,\n",
              " 836,\n",
              " 837,\n",
              " 838,\n",
              " 839,\n",
              " 840,\n",
              " 841,\n",
              " 842,\n",
              " 843,\n",
              " 844,\n",
              " 845,\n",
              " 846,\n",
              " 847,\n",
              " 848,\n",
              " 849,\n",
              " 850,\n",
              " 851,\n",
              " 852,\n",
              " 853,\n",
              " 854,\n",
              " 855,\n",
              " 856,\n",
              " 857,\n",
              " 858,\n",
              " 859,\n",
              " 860,\n",
              " 861,\n",
              " 862,\n",
              " 863,\n",
              " 864,\n",
              " 865,\n",
              " 866,\n",
              " 867,\n",
              " 868,\n",
              " 869,\n",
              " 870,\n",
              " 871,\n",
              " 872,\n",
              " 873,\n",
              " 874,\n",
              " 875,\n",
              " 876,\n",
              " 877,\n",
              " 878,\n",
              " 879,\n",
              " 880,\n",
              " 881,\n",
              " 882,\n",
              " 883,\n",
              " 884,\n",
              " 885,\n",
              " 886,\n",
              " 887,\n",
              " 888,\n",
              " 889,\n",
              " 890,\n",
              " 891,\n",
              " 892,\n",
              " 893,\n",
              " 894,\n",
              " 895,\n",
              " 896,\n",
              " 897,\n",
              " 898,\n",
              " 899,\n",
              " 900,\n",
              " 901,\n",
              " 902,\n",
              " 903,\n",
              " 904,\n",
              " 905,\n",
              " 906,\n",
              " 907,\n",
              " 908,\n",
              " 909,\n",
              " 910,\n",
              " 911,\n",
              " 912,\n",
              " 913,\n",
              " 914,\n",
              " 915,\n",
              " 916,\n",
              " 917,\n",
              " 918,\n",
              " 919,\n",
              " 920,\n",
              " 921,\n",
              " 922,\n",
              " 923,\n",
              " 924,\n",
              " 925,\n",
              " 926,\n",
              " 927,\n",
              " 928,\n",
              " 929,\n",
              " 930,\n",
              " 931,\n",
              " 932,\n",
              " 933,\n",
              " 934,\n",
              " 935,\n",
              " 936,\n",
              " 937,\n",
              " 938,\n",
              " 939,\n",
              " 940,\n",
              " 941,\n",
              " 942,\n",
              " 943,\n",
              " 944,\n",
              " 945,\n",
              " 946,\n",
              " 947,\n",
              " 948,\n",
              " 949,\n",
              " 950,\n",
              " 951,\n",
              " 952,\n",
              " 953,\n",
              " 954,\n",
              " 955,\n",
              " 956,\n",
              " 957,\n",
              " 958,\n",
              " 959,\n",
              " 960,\n",
              " 961,\n",
              " 962,\n",
              " 963,\n",
              " 964,\n",
              " 965,\n",
              " 966,\n",
              " 967,\n",
              " 968,\n",
              " 969,\n",
              " 970,\n",
              " 971,\n",
              " 972,\n",
              " 973,\n",
              " 974,\n",
              " 975,\n",
              " 976,\n",
              " 977,\n",
              " 978,\n",
              " 979,\n",
              " 980,\n",
              " 981,\n",
              " 982,\n",
              " 983,\n",
              " 984,\n",
              " 985,\n",
              " 986,\n",
              " 987,\n",
              " 988,\n",
              " 989,\n",
              " 990,\n",
              " 991,\n",
              " 992,\n",
              " 993,\n",
              " 994,\n",
              " 995,\n",
              " 996,\n",
              " 997,\n",
              " 998,\n",
              " 999}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}